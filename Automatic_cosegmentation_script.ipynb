{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###1. Start by installing the Personalised Segment Anything Model from Hugging face"
      ],
      "metadata": {
        "id": "CWWKSC6zbtgZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "d10ffae93aa04746b41ee4ee81f278a4",
            "e6c6480c9b2441af9b903c65a5d6cf2c",
            "179f4f7c03854baca5db9f4afedbf560",
            "cfb9b66a73934b95af3da7025cc7f1c6",
            "2168996fe94e491e83d61da98bb22843",
            "2864d7864dee49caa494999e4d992d24",
            "f7dd1ccdddcb45efb83e190a884d3366",
            "c8873cbfa52a46be8143df7130957866",
            "22c4a33df36f48149350a88809ddd0e0",
            "37d86c30cb3c4644af6dbf4b4a4dc5ca",
            "63027011d8024f9680b9ffbae6f344ec",
            "6f79b832b8204111967517da93c4a263",
            "ece7500d65e844a49683bf6b4374f3d6",
            "a2976df6649f416b9c9fcdd0a7370aad",
            "3844e18c87604cbb974fcb14c7460e53",
            "f5759a04390445089d339c9e20453698",
            "1d9456e94af644adbfa5334163e3c478",
            "c9275e44f29b4a57b9692a454247726c",
            "d60fadf7c2a04fd09e74502f82b6806c",
            "383011f39a6448a09a7d76b7178f3b2f",
            "7e916858f179471b85579baa9407e9ba",
            "46d6e9a021fb4682bccf61d52f8076c3",
            "522cd35a9c59403999b8e5e793226945",
            "df9ec6456a09491da2ae39099facebe6",
            "8f52cf77b134456486d0fef740750459",
            "5e70354cfae0494999ffa158a09cd2ba",
            "0779502b51214262912eaec3eac732a6",
            "2d4f70a41aed44918cd2c0dec4b87f4d",
            "ef02f947198041eea185793acb7d3dcd",
            "76b4fa248f4243dc99e9af166ebeded7",
            "6d04d1d153b24352a6a498967ba86290",
            "da8a3e3861834a6cbff9ef2bf7839dce",
            "6724291c26e149908db4d9bc02242bf1"
          ]
        },
        "collapsed": true,
        "id": "tjO71hTi3f8V",
        "outputId": "39be50c6-ac5e-46e4-eb18-e36c16f32cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d10ffae93aa04746b41ee4ee81f278a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/6.57k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f79b832b8204111967517da93c4a263"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.56G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "522cd35a9c59403999b8e5e793226945"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "from transformers import AutoProcessor, SamModel\n",
        "# from transformers import PerSamModel\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
        "# model = PerSamModel.from_pretrained(\"facebook/sam-vit-huge\")\n",
        "model = SamModel.from_pretrained(\"facebook/sam-vit-huge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import Necessary python libraries"
      ],
      "metadata": {
        "id": "xPrf9sW7cG1G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMww_o3o2h46"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchvision.transforms.functional import resize, to_pil_image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from transformers import AutoProcessor, SamModel\n",
        "# from transformers import PerSamModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. You can try mounting your google drive where additional models can be downloaded and imported directly into the collab"
      ],
      "metadata": {
        "id": "kzgkIkZMcbVO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYluFa3CpUw_",
        "outputId": "49bdfce4-0e9f-4579-f853-2f5794c6be65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is where helper fucntions were created for further simplification of the code"
      ],
      "metadata": {
        "id": "S_LixYy7c5dK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUebVEEXut89"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.cluster import KMeans\n",
        "from huggingface_hub import hf_hub_download\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms.functional import resize, to_pil_image\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Function to load images from a directory\n",
        "def load_images_from_directory(directory):\n",
        "    images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            img = Image.open(os.path.join(directory, filename)).convert(\"RGB\")\n",
        "            if img is not None:\n",
        "                images.append((filename, img))\n",
        "    return images\n",
        "\n",
        "# Function to extract features from an image using spatial average\n",
        "def get_image_feature(ref_image):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Step 1: Image features encoding\n",
        "    pixel_values = processor(images=ref_image, return_tensors=\"pt\").pixel_values\n",
        "    with torch.no_grad():\n",
        "        ref_feat = model.get_image_embeddings(pixel_values.to(device))\n",
        "        ref_feat = ref_feat.squeeze().permute(1, 2, 0)\n",
        "\n",
        "    # Compute the mean across the height and width dimensions\n",
        "    spatial_average = ref_feat.mean(dim=(0, 1))\n",
        "\n",
        "    # Reshape to (1, 1, C)\n",
        "    spatial_average = spatial_average.view(1, 1, -1)\n",
        "\n",
        "    return spatial_average\n",
        "\n",
        "# Function to count the number of images in a directory\n",
        "def count(directory):\n",
        "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif'}\n",
        "    image_count = sum(1 for filename in os.listdir(directory) if filename.lower().endswith(tuple(valid_extensions)))\n",
        "    return image_count\n",
        "\n",
        "# Segment images using k-means clustering\n",
        "def segment_images(image_direc,cluster_direc, images, k):\n",
        "    # Extract features from all images\n",
        "    features = [get_image_feature(img).cpu().numpy().flatten() for _, img in images]\n",
        "\n",
        "    # Perform k-means clustering\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0).fit(features)\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    # Create directory for each cluster\n",
        "    for i in range(k):\n",
        "        os.makedirs(os.path.join(cluster_direc, f'cluster_{i}'), exist_ok=True)\n",
        "\n",
        "    # Save images to corresponding cluster directory\n",
        "    # for (filename, img), label in zip(images, labels):\n",
        "    #     img.save(os.path.join(image_direc, f'cluster_{label}', filename))\n",
        "    cluster_log = {i: [] for i in range(k)}\n",
        "    for (filename, img), label in zip(images, labels):\n",
        "        img.save(os.path.join(cluster_direc, f'cluster_{label}', filename))\n",
        "        cluster_log[label].append(filename)\n",
        "\n",
        "    return labels,cluster_log\n",
        "\n",
        "\n",
        "\n",
        "import shutil\n",
        "\n",
        "def clusters(cluster_log, image_direc, saliency_direc, output_direc):\n",
        "    # Iterate over each cluster and its associated filenames\n",
        "    for cluster, filenames in cluster_log.items():\n",
        "        # Create the cluster directory in the output directory if it doesn't exist\n",
        "        cluster_output_dir = os.path.join(output_direc, f'cluster_{cluster}')\n",
        "        os.makedirs(cluster_output_dir, exist_ok=True)\n",
        "\n",
        "        for filename in filenames:\n",
        "            # Construct the image path in the image directory\n",
        "            # img_path = os.path.join(image_direc, f'cluster_{cluster}', filename)\n",
        "            img_path = os.path.join(image_direc, filename)\n",
        "            # Open the image to ensure it exists and is accessible\n",
        "            try:\n",
        "                image = Image.open(img_path)\n",
        "            except FileNotFoundError:\n",
        "                print(f\"Image {filename} not found in {img_path}. Skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Determine the corresponding saliency map filename\n",
        "            base_filename = os.path.splitext(filename)[0]\n",
        "            saliency_filename = base_filename + '.png'\n",
        "            saliency_path = os.path.join(saliency_direc, saliency_filename)\n",
        "\n",
        "            # Check if the saliency map exists\n",
        "            if os.path.exists(saliency_path):\n",
        "                # Move or copy the saliency map to the corresponding cluster folder in the output directory\n",
        "                target_path = os.path.join(cluster_output_dir, saliency_filename)\n",
        "                shutil.copy(saliency_path, target_path)\n",
        "            else:\n",
        "                print(f\"Saliency map {saliency_filename} not found in {saliency_direc}. Skipping...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# clusters(cluster_log, 'path/to/image_directory', 'path/to/saliency_directory', 'path/to/output_directory')\n",
        "\n",
        "\n",
        "# Main function to execute the workflow\n",
        "def main(image_directory,cluster_direc, k):\n",
        "    images = load_images_from_directory(image_directory)\n",
        "    labels,cluster_log = segment_images(image_directory,cluster_direc, images, k)\n",
        "    return labels,cluster_log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JbMKvJWHnsqx",
        "outputId": "bdb9e2e2-195c-4936-a941-f0da49da75c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SamModel(\n",
              "  (shared_image_embedding): SamPositionalEmbedding()\n",
              "  (vision_encoder): SamVisionEncoder(\n",
              "    (patch_embed): SamPatchEmbeddings(\n",
              "      (projection): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x SamVisionLayer(\n",
              "        (layer_norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): SamVisionAttention(\n",
              "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): SamMLPBlock(\n",
              "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (neck): SamVisionNeck(\n",
              "      (conv1): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (layer_norm1): SamLayerNorm()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (layer_norm2): SamLayerNorm()\n",
              "    )\n",
              "  )\n",
              "  (prompt_encoder): SamPromptEncoder(\n",
              "    (shared_embedding): SamPositionalEmbedding()\n",
              "    (mask_embed): SamMaskEmbedding(\n",
              "      (activation): GELUActivation()\n",
              "      (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (conv2): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (conv3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (layer_norm1): SamLayerNorm()\n",
              "      (layer_norm2): SamLayerNorm()\n",
              "    )\n",
              "    (no_mask_embed): Embedding(1, 256)\n",
              "    (point_embed): ModuleList(\n",
              "      (0-3): 4 x Embedding(1, 256)\n",
              "    )\n",
              "    (not_a_point_embed): Embedding(1, 256)\n",
              "  )\n",
              "  (mask_decoder): SamMaskDecoder(\n",
              "    (iou_token): Embedding(1, 256)\n",
              "    (mask_tokens): Embedding(4, 256)\n",
              "    (transformer): SamTwoWayTransformer(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x SamTwoWayAttentionBlock(\n",
              "          (self_attn): SamAttention(\n",
              "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (cross_attn_token_to_image): SamAttention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): SamMLPBlock(\n",
              "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (act): ReLU()\n",
              "          )\n",
              "          (layer_norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (layer_norm4): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (cross_attn_image_to_token): SamAttention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_attn_token_to_image): SamAttention(\n",
              "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "      )\n",
              "      (layer_norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (upscale_conv1): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (upscale_conv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (upscale_layer_norm): SamLayerNorm()\n",
              "    (activation): GELU(approximate='none')\n",
              "    (output_hypernetworks_mlps): ModuleList(\n",
              "      (0-3): 4 x SamFeedForward(\n",
              "        (activation): ReLU()\n",
              "        (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (proj_out): Linear(in_features=256, out_features=32, bias=True)\n",
              "        (layers): ModuleList(\n",
              "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (iou_prediction_head): SamFeedForward(\n",
              "      (activation): ReLU()\n",
              "      (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (proj_out): Linear(in_features=256, out_features=4, bias=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms.functional import resize, to_pil_image\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "from skimage.measure import label, regionprops\n",
        "from torchvision.transforms.functional import resize, to_pil_image\n",
        "\n",
        "def prepare_mask(image, target_length=1024):\n",
        "    target_size = get_preprocess_shape(image.shape[0], image.shape[1], target_length)\n",
        "    mask = np.array(resize(to_pil_image(image), target_size))\n",
        "\n",
        "    if len(mask.shape) == 2:\n",
        "        mask = mask[:, :, None]\n",
        "\n",
        "    input_mask = torch.as_tensor(mask)\n",
        "    input_mask = input_mask.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
        "    input_mask = preprocess(input_mask)\n",
        "    return input_mask\n",
        "\n",
        "# def point_selection(mask_sim, topk=1):\n",
        "#     w, h = mask_sim.shape\n",
        "#     topk_xy = mask_sim.flatten(0).topk(topk)[1]\n",
        "#     topk_x = (topk_xy // h).unsqueeze(0)\n",
        "#     topk_y = (topk_xy - topk_x * h)\n",
        "#     topk_xy = torch.cat((topk_y, topk_x), dim=0).permute(1, 0)\n",
        "#     topk_label = np.array([1] * topk)\n",
        "#     topk_xy = topk_xy.cpu().numpy()\n",
        "#     last_xy = mask_sim.flatten(0).topk(topk, largest=False)[1]\n",
        "#     last_x = (last_xy // h).unsqueeze(0)\n",
        "#     last_y = (last_xy - last_x * h)\n",
        "#     last_xy = torch.cat((last_y, last_x), dim=0).permute(1, 0)\n",
        "#     last_label = np.array([0] * topk)\n",
        "#     last_xy = last_xy.cpu().numpy()\n",
        "#     return topk_xy, topk_label, last_xy, last_label\n",
        "\n",
        "\n",
        "def point_selection22(sim,original_image):\n",
        "    attention_similarity = sim.sigmoid_().unsqueeze(0)\n",
        "    threshold = 0.7\n",
        "    binary_mask = (attention_similarity > threshold).float()\n",
        "\n",
        "              # Ensure the mask is 2D\n",
        "    binary_mask_np = binary_mask.squeeze(0).cpu().numpy().astype(np.uint8)\n",
        "    # print(f\"binary_mask_np shape: {binary_mask_np.shape}, dtype: {binary_mask_np.dtype}\")\n",
        "\n",
        "      #Perform connected component analysis\n",
        "    labeled_mask, num_labels = label(binary_mask_np, return_num=True)\n",
        "    # print(f\"labeled_mask shape: {labeled_mask.shape}, dtype: {labeled_mask.dtype}, num_labels: {num_labels}\")\n",
        "\n",
        "    #Find the size of the largest connected component\n",
        "    max_region_area = max(region.area for region in regionprops(labeled_mask))\n",
        "    size_threshold = max_region_area * 0.1\n",
        "\n",
        "    #Filter connected components based on size and find centroids\n",
        "    final_mask = np.zeros_like(labeled_mask)\n",
        "    centroids = []\n",
        "    for region in regionprops(labeled_mask):\n",
        "        if region.area >= size_threshold:  # Only keep regions that are at least 10% the size of the largest region\n",
        "          centroids.append((region.centroid[0], region.centroid[1]))\n",
        "          for coords in region.coords:\n",
        "            final_mask[coords[0], coords[1]] = 1\n",
        "\n",
        "    if isinstance(original_image, Image.Image) and original_image.size:\n",
        "        # Resize final mask to original image size\n",
        "        original_size = original_image.size[::-1]  # PIL Image size is (width, height), so reverse\n",
        "        print(\"Original image size:\", original_image.size)\n",
        "        print(\"final_mask size:\", final_mask.size)\n",
        "        if original_size:\n",
        "          final_mask_2d = final_mask.reshape((64, 64))\n",
        "          final_mask_resized = cv2.resize(final_mask_2d, original_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Convert centroids to original image scale\n",
        "          scale_factor_y = original_size[0] / final_mask.shape[0]\n",
        "          scale_factor_x = original_size[1] / final_mask.shape[1]\n",
        "          # centroids = [tuple(region.centroid) for region in regionprops(labeled_mask) if region.area >= size_threshold]\n",
        "          centroids = [(int(y * scale_factor_y), int(x * scale_factor_x)) for y, x in centroids]\n",
        "          print(\"Centroid  \", centroids)\n",
        "          topk_coords = np.array(centroids)[:len(centroids)]\n",
        "          bottomk_coords = np.array(centroids)[:0]\n",
        "\n",
        "          topk_labels = np.array([1] * len(centroids))\n",
        "          bottomk_labels = np.array([0] * 0)\n",
        "          print(\"Top K Coordinates: \", topk_coords)\n",
        "          print(\"Top K Labels: \", topk_labels)\n",
        "          print(\"Bottom K Coordinates: \", bottomk_coords)\n",
        "          print(\"Bottom K Labels: \", bottomk_labels)\n",
        "          return topk_coords, topk_labels, bottomk_coords, bottomk_labels\n",
        "        else:\n",
        "          print(\"Error: Original image has invalid size. Cannot resize mask.\")\n",
        "          return None, None, None, None  # Handle the error as needed\n",
        "\n",
        "    else:\n",
        "        print(\"Error: Invalid original image. Unable to resize mask.\")\n",
        "        return None, None, None, None # or handle the error appropriately\n",
        "\n",
        "def preprocess(x: torch.Tensor, pixel_mean=[123.675, 116.28, 103.53], pixel_std=[58.395, 57.12, 57.375], img_size=1024) -> torch.Tensor:\n",
        "    pixel_mean = torch.Tensor(pixel_mean).view(-1, 1, 1)\n",
        "    pixel_std = torch.Tensor(pixel_std).view(-1, 1, 1)\n",
        "    x = (x - pixel_mean) / pixel_std\n",
        "    h, w = x.shape[-2:]\n",
        "    padh = img_size - h\n",
        "    padw = img_size - w\n",
        "    x = F.pad(x, (0, padw, 0, padh))\n",
        "    return x\n",
        "\n",
        "def get_preprocess_shape(oldh: int, oldw: int, long_side_length: int) -> Tuple[int, int]:\n",
        "    scale = long_side_length * 1.0 / max(oldh, oldw)\n",
        "    newh, neww = oldh * scale, oldw * scale\n",
        "    neww = int(neww + 0.5)\n",
        "    newh = int(newh + 0.5)\n",
        "    return (newh, neww)\n",
        "\n",
        "\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([255, 0, 0, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Importing the Segment Anything model for using its decoder after the point prompts are generated\n"
      ],
      "metadata": {
        "id": "SZoDCcfDdHhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "    !mkdir images\n",
        "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
        "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n",
        "\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce-B_cNFjGE5",
        "outputId": "a151e5b2-387f-40ab-9c8c-bddb92201afb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.3.0+cu121\n",
            "Torchvision version: 0.18.0+cu121\n",
            "CUDA is available: True\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-9t06qzlq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-9t06qzlq\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment-anything\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36590 sha256=f801af85f95e61b635a61c503a8a4bb87104552143dca3adcb0ea98b857760ea\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yq4d779u/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "Successfully built segment-anything\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n",
            "--2024-06-17 02:22:22--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 271475 (265K) [image/jpeg]\n",
            "Saving to: ‘images/truck.jpg’\n",
            "\n",
            "truck.jpg           100%[===================>] 265.11K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2024-06-17 02:22:23 (64.1 MB/s) - ‘images/truck.jpg’ saved [271475/271475]\n",
            "\n",
            "--2024-06-17 02:22:23--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168066 (164K) [image/jpeg]\n",
            "Saving to: ‘images/groceries.jpg’\n",
            "\n",
            "groceries.jpg       100%[===================>] 164.13K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-06-17 02:22:23 (6.61 MB/s) - ‘images/groceries.jpg’ saved [168066/168066]\n",
            "\n",
            "--2024-06-17 02:22:23--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.121, 108.157.254.15, 108.157.254.124, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_h_4b8939.pth’\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   235MB/s    in 11s     \n",
            "\n",
            "2024-06-17 02:22:34 (220 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Load the SAM model and build necesary fucntions"
      ],
      "metadata": {
        "id": "3u04bzead2c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "id": "7WtBqFVFi1gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
      ],
      "metadata": {
        "id": "ImEjzY-sj9HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to create few directories where our output would be stored."
      ],
      "metadata": {
        "id": "DiXpmm0CeHUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir  '/content/IMAGES_IN_CLUSTERS/14'\n",
        "!mkdir  '/content/OUT-PUT-MASK/14'\n",
        "!mkdir  '/content/SALIENCY_IN_CLUSTERS/14'\n"
      ],
      "metadata": {
        "id": "Gq-6M-T3fEml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11dbd880-98fb-4be6-dd2c-1583175a5f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/OUT-PUT-MASK/14’: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IKuDyLJaG69G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c37f78-7cc7-43dd-d878-94610ad44b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0]\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.png\n",
            "binary_mask_np shape: (375, 500), dtype: uint8\n",
            "labeled_mask shape: (375, 500), dtype: int64, num_labels: 14\n",
            "[[251 224]]\n",
            "tensor([[ 64, 114, 401, 337]], device='cuda:0')\n",
            "Saved visualization to: /content/drive/MyDrive/OUT-PUT-MASK(BB)/18/414841074_42891c2e75.jpg\n",
            "Saved binary mask to: /content/drive/MyDrive/OUT-PUT-MASK(BB)/18/414841074_42891c2e75.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.png\n",
            "binary_mask_np shape: (500, 368), dtype: uint8\n",
            "labeled_mask shape: (500, 368), dtype: int64, num_labels: 27\n",
            "[[138 184]\n",
            " [337 189]\n",
            " [439  92]]\n",
            "tensor([[112,  53, 257, 208],\n",
            "        [130, 258, 238, 445],\n",
            "        [ 12, 342, 163, 500]], device='cuda:0')\n",
            "Saved visualization to: /content/drive/MyDrive/OUT-PUT-MASK(BB)/18/442102966_f5e76ea887.jpg\n",
            "Saved binary mask to: /content/drive/MyDrive/OUT-PUT-MASK(BB)/18/442102966_f5e76ea887.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.png\n",
            "binary_mask_np shape: (500, 375), dtype: uint8\n",
            "labeled_mask shape: (500, 375), dtype: int64, num_labels: 21\n",
            "[[211 208]\n",
            " [234 336]\n",
            " [344 316]\n",
            " [408 189]]\n",
            "tensor([[109, 128, 292, 267],\n",
            "        [308, 197, 364, 275],\n",
            "        [275, 322, 356, 365],\n",
            "        [ 45, 353, 353, 451]], device='cuda:0')\n",
            "Saved visualization to: /content/drive/MyDrive/OUT-PUT-MASK(BB)/18/442103094_b160d16d19.jpg\n",
            "Saved binary mask to: /content/drive/MyDrive/OUT-PUT-MASK(BB)/18/442103094_b160d16d19.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.png\n",
            "binary_mask_np shape: (375, 500), dtype: uint8\n",
            "labeled_mask shape: (375, 500), dtype: int64, num_labels: 36\n",
            "[[127 261]\n",
            " [126 413]\n",
            " [345 368]\n",
            " [347  96]]\n",
            "tensor([[180,  35, 343, 198],\n",
            "        [398,  58, 423, 192],\n",
            "        [241, 306, 495, 367],\n",
            "        [  0, 315, 190, 368]], device='cuda:0')\n",
            "Saved visualization to: /content/drive/MyDrive/OUT-PUT-MASK(BB)/18/442104314_b6f836888b.jpg\n",
            "Saved binary mask to: /content/drive/MyDrive/OUT-PUT-MASK(BB)/18/442104314_b6f836888b.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/414841074_42891c2e75.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442102966_f5e76ea887.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442103094_b160d16d19.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442104314_b6f836888b.png\n",
            "Processing: Test Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.jpg, Reference Image: /content/IMAGES_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.jpg, Reference Mask: /content/SALIENCY_IN_CLUSTERS/18/cluster_0/442105753_f45e375afc.png\n",
            "binary_mask_np shape: (375, 500), dtype: uint8\n",
            "labeled_mask shape: (375, 500), dtype: int64, num_labels: 8\n",
            "[[236 238]]\n",
            "tensor([[  0,  62, 461, 332]], device='cuda:0')\n",
            "Saved visualization to: /content/drive/MyDrive/OUT-PUT-MASK(BB)/18/442105753_f45e375afc.jpg\n",
            "Saved binary mask to: /content/drive/MyDrive/OUT-PUT-MASK(BB)/18/442105753_f45e375afc.png\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "from skimage.measure import label, regionprops\n",
        "\n",
        "# Directory setup\n",
        "image_direct = '/content/drive/MyDrive/SURGEIMAGEfolder/images/018 Agra Taj Mahal-Inde du Nord 2004-Mhln'\n",
        "saliency_direc = '/content/drive/MyDrive/TOTAL-SALIENCY/18'\n",
        "!mkdir -p '/content/SALIENCY_IN_CLUSTERS/18'\n",
        "saliency_map_cluster_direc = '/content/SALIENCY_IN_CLUSTERS/18'\n",
        "!mkdir -p '/content/IMAGES_IN_CLUSTERS/18'\n",
        "image_cluster_direc = '/content/IMAGES_IN_CLUSTERS/18'\n",
        "!mkdir -p '/content/drive/MyDrive/OUT-PUT-MASK(BB)/18'\n",
        "output_directory = '/content/drive/MyDrive/OUT-PUT-MASK(BB)/18'\n",
        "\n",
        "k = math.ceil(count(image_direct) / 10)  # Number of clusters\n",
        "\n",
        "labels, cluster_log = main(image_direct, image_cluster_direc, k)\n",
        "print(labels)\n",
        "clusters(cluster_log, image_direct, saliency_direc, saliency_map_cluster_direc)\n",
        "\n",
        "\n",
        "\n",
        "image_clusters = [d for d in os.listdir(image_cluster_direc) if os.path.isdir(os.path.join(image_cluster_direc, d))]\n",
        "# List all image cluster folders\n",
        "for image_cluster in image_clusters:\n",
        "    # Construct the corresponding saliency map cluster directory\n",
        "    saliency_map_cluster = os.path.join(saliency_map_cluster_direc, image_cluster)\n",
        "\n",
        "    # Check if the corresponding saliency map cluster folder exists\n",
        "    if os.path.isdir(saliency_map_cluster):\n",
        "        image_cluster_path = os.path.join(image_cluster_direc, image_cluster)\n",
        "\n",
        "        image_files = sorted([f for f in os.listdir(image_cluster_path) if f.endswith(\".jpg\") or f.endswith(\".png\")])\n",
        "        image_files = [os.path.join(image_cluster_path, f) for f in image_files]  # Include full path\n",
        "\n",
        "        mask_files = sorted([f for f in os.listdir(saliency_map_cluster) if f.endswith(\".jpg\") or f.endswith(\".png\")])\n",
        "        mask_files = [os.path.join(saliency_map_cluster, f) for f in mask_files]  # Include full path\n",
        "\n",
        "        for test_image_filename in image_files:\n",
        "            if test_image_filename.endswith(\".jpg\") or test_image_filename.endswith(\".png\"):\n",
        "                file_path = os.path.join(image_cluster_path, test_image_filename)\n",
        "                test_image = Image.open(file_path).convert(\"RGB\")\n",
        "\n",
        "                # Prepare test image\n",
        "                inputs = processor(images=test_image, return_tensors=\"pt\").to(device)\n",
        "                pixel_values = inputs.pixel_values\n",
        "                with torch.no_grad():\n",
        "                    test_feat = model.get_image_embeddings(pixel_values).squeeze()\n",
        "\n",
        "                num_channels, height, width = test_feat.shape\n",
        "                test_feat = test_feat / test_feat.norm(dim=0, keepdim=True)\n",
        "                test_feat_reshaped = test_feat.reshape(num_channels, height * width)\n",
        "\n",
        "                # Initialize a list to store similarity matrices\n",
        "                sims_list = []\n",
        "\n",
        "                # Iterate over reference images\n",
        "                for ref_image_filename, ref_mask_filename in zip(image_files, mask_files):\n",
        "                    ref_image_path = os.path.join(image_cluster_path, ref_image_filename)\n",
        "                    ref_mask_path = os.path.join(saliency_map_cluster, ref_mask_filename)\n",
        "\n",
        "                    # Load reference image and mask\n",
        "                    ref_image = Image.open(ref_image_path).convert(\"RGB\")\n",
        "                    ref_mask = cv2.imread(ref_mask_path)\n",
        "                    ref_mask = cv2.cvtColor(ref_mask, cv2.COLOR_BGR2RGB)\n",
        "                    np.unique(ref_mask)\n",
        "\n",
        "                    # Precompute reference image features and mask processing\n",
        "                    pixel_values = processor(images=ref_image, return_tensors=\"pt\").pixel_values\n",
        "                    with torch.no_grad():\n",
        "                        ref_feat = model.get_image_embeddings(pixel_values.to(device))\n",
        "                        ref_feat = ref_feat.squeeze().permute(1, 2, 0)\n",
        "\n",
        "                    ref_mask = prepare_mask(ref_mask)\n",
        "                    ref_mask = F.interpolate(ref_mask, size=ref_feat.shape[0:2], mode=\"bilinear\")\n",
        "                    ref_mask = ref_mask.squeeze()[0]\n",
        "                    target_feat = ref_feat[ref_mask > 0]\n",
        "                    target_embedding = target_feat.mean(0).unsqueeze(0)\n",
        "                    target_feat = target_embedding / target_embedding.norm(dim=-1, keepdim=True)\n",
        "                    target_embedding = target_embedding.unsqueeze(0)\n",
        "\n",
        "                    # Compute similarity\n",
        "                    sim = target_feat @ test_feat_reshaped\n",
        "                    sim = sim.reshape(1, 1, height, width)\n",
        "                    sim = F.interpolate(sim, scale_factor=4, mode=\"bilinear\")\n",
        "                    sim = processor.post_process_masks(sim.unsqueeze(1), original_sizes=inputs[\"original_sizes\"].tolist(), reshaped_input_sizes=inputs[\"reshaped_input_sizes\"].tolist(), binarize=False)\n",
        "                    sim = sim[0].squeeze()\n",
        "                    sims_list.append(sim)\n",
        "                    print(f\"Processing: Test Image: {test_image_filename}, Reference Image: {ref_image_filename}, Reference Mask: {ref_mask_filename}\")\n",
        "\n",
        "                # Compute the average similarity matrix\n",
        "                avg_sim = torch.mean(torch.stack(sims_list), dim=0)\n",
        "\n",
        "                if avg_sim.max() > 1.0:\n",
        "                    avg_sim = avg_sim / avg_sim.max()\n",
        "\n",
        "                sim = avg_sim\n",
        "                sim = (sim - sim.mean()) / torch.std(sim)\n",
        "                # sim = F.interpolate(sim.unsqueeze(0).unsqueeze(0), size=(height, width), mode=\"bilinear\")  # Adjusted size here\n",
        "                attention_similarity = sim.sigmoid_().unsqueeze(0)\n",
        "\n",
        "\n",
        "                threshold = 0.69\n",
        "                binary_mask = (attention_similarity > threshold).float()\n",
        "\n",
        "# Ensure the mask is 2D\n",
        "                binary_mask_np = binary_mask.squeeze().cpu().numpy().astype(np.uint8)\n",
        "                print(f\"binary_mask_np shape: {binary_mask_np.shape}, dtype: {binary_mask_np.dtype}\")\n",
        "\n",
        "# Perform connected component analysis\n",
        "                labeled_mask, num_labels = label(binary_mask_np, return_num=True)\n",
        "                print(f\"labeled_mask shape: {labeled_mask.shape}, dtype: {labeled_mask.dtype}, num_labels: {num_labels}\")\n",
        "\n",
        "# Find the size of the largest connected component\n",
        "                max_region_area = max(region.area for region in regionprops(labeled_mask))\n",
        "                size_threshold = max_region_area * 0.1\n",
        "\n",
        "# Filter connected components based on size and find centroids and bounding boxes\n",
        "                final_mask = np.zeros_like(labeled_mask)\n",
        "                centroids = []\n",
        "                bounding_boxes = []\n",
        "                for region in regionprops(labeled_mask):\n",
        "                    if region.area >= size_threshold:  # Only keep regions that are at least 10% the size of the largest region\n",
        "                        centroids.append(region.centroid)\n",
        "                        bounding_boxes.append(region.bbox)\n",
        "                        for coords in region.coords:\n",
        "                            final_mask[coords[0], coords[1]] = 1\n",
        "\n",
        "# Resize final mask to original image size\n",
        "                original_size = test_image.size[::-1]  # PIL Image size is (width, height), so reverse\n",
        "                final_mask_resized = cv2.resize(final_mask, original_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "# Convert centroids to original image scale\n",
        "                scale_factor_y = original_size[0] / final_mask.shape[0]\n",
        "                scale_factor_x = original_size[1] / final_mask.shape[1]\n",
        "                centroids_original_scale = [(int(y * scale_factor_y), int(x * scale_factor_x)) for y, x in centroids]\n",
        "\n",
        "# Print or save the centroids\n",
        "                topk_xy_i = np.array(centroids_original_scale)[:len(centroids_original_scale)]\n",
        "                topk_label_i = np.array([1] * len(centroids_original_scale))\n",
        "\n",
        "                topk_xy = np.concatenate([topk_xy_i], axis=0)\n",
        "                topk_label = np.concatenate([topk_label_i], axis=0)\n",
        "\n",
        "                print(topk_xy)\n",
        "\n",
        "# Convert bounding boxes to original image scale\n",
        "                bounding_boxes_original_scale = []\n",
        "                for bbox in bounding_boxes:\n",
        "                    min_row, min_col, max_row, max_col = bbox\n",
        "                    x1 = int(min_col * scale_factor_x)\n",
        "                    y1 = int(min_row * scale_factor_y)\n",
        "                    x2 = int(max_col * scale_factor_x)\n",
        "                    y2 = int(max_row * scale_factor_y)\n",
        "                    bounding_boxes_original_scale.append([x1, y1, x2, y2])\n",
        "\n",
        "# Convert bounding boxes to tensor\n",
        "                input_boxes = bounding_boxes_original_scale\n",
        "                input_boxes = torch.tensor(input_boxes, device=device)  # Add this line\n",
        "\n",
        "\n",
        "# Print the input boxes\n",
        "                print(input_boxes)\n",
        "\n",
        "# Prepare the input points and labels for the predictor\n",
        "                test_image_np = np.array(test_image)\n",
        "                predictor.set_image(test_image_np)\n",
        "\n",
        "                input_point = topk_xy_i\n",
        "                input_point = input_point[:, ::-1]\n",
        "                input_label = topk_label_i\n",
        "# Create a copy of input_point before converting to tensor\n",
        "                input_point_tensor = torch.tensor(input_point.copy(), device=device)\n",
        "\n",
        "\n",
        "                transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, test_image_np.shape[:2])\n",
        "\n",
        "                masks, _, _ = predictor.predict_torch(\n",
        "                point_coords=None,  # Use the tensor here\n",
        "                point_labels=None,\n",
        "                boxes=transformed_boxes,\n",
        "                multimask_output=False,\n",
        "                )\n",
        "\n",
        "                # masks, scores, logits = predictor.predict(\n",
        "                #     point_coords=input_point,\n",
        "                #     point_labels=input_label,\n",
        "                #     multimask_output=True,\n",
        "                # )\n",
        "\n",
        "                def show_points(coords, labels, ax, marker_size=375):\n",
        "                    pos_points = coords[labels==1]\n",
        "                    neg_points = coords[labels==0]\n",
        "                    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "                    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(10, 10))\n",
        "                ax.imshow(test_image)\n",
        "\n",
        "# Combine masks and display them\n",
        "                combined_mask = np.zeros(test_image_np.shape[:2], dtype=np.uint8)\n",
        "                for mask in masks:\n",
        "                  mask_np = mask.cpu().numpy().squeeze()\n",
        "                  combined_mask = np.maximum(combined_mask, mask_np)\n",
        "                  show_mask(mask_np, ax, random_color=True)\n",
        "\n",
        "# Display boxes\n",
        "                for box in input_boxes:\n",
        "                  show_box(box.cpu().numpy(), ax)\n",
        "\n",
        "# Remove axis\n",
        "\n",
        "\n",
        "# Save the figure with the masks and boxes\n",
        "                output_path = os.path.join(output_directory, os.path.basename(test_image_filename))\n",
        "                plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
        "                plt.close(fig)\n",
        "                print(f\"Saved visualization to: {output_path}\")\n",
        "\n",
        "# Convert the selected mask to a binary mask\n",
        "                binary_mask = combined_mask.astype(np.uint8)\n",
        "\n",
        "# Convert binary mask to PIL image\n",
        "                binary_mask_pil = Image.fromarray(binary_mask * 255)\n",
        "\n",
        "# Save the binary mask\n",
        "                binary_mask_output_path = os.path.join(output_directory, os.path.basename(test_image_filename).replace('.jpg', '.png'))\n",
        "                binary_mask_pil.save(binary_mask_output_path)\n",
        "                print(f\"Saved binary mask to: {binary_mask_output_path}\")\n",
        "\n",
        "# --------new above --------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # all_masks = []\n",
        "                # all_scores = []\n",
        "\n",
        "                # for points, labels, box in zip(points_sets, labels_sets, boxes):\n",
        "                #       masks, scores, _ = predictor.predict(point_coords=points, point_labels=labels, box=box)\n",
        "                #       all_masks.append(masks)\n",
        "                #       all_scores.append(scores)\n",
        "                # plt.figure(figsize=(10, 10))\n",
        "                # plt.imshow(test_image)\n",
        "                # show_mask(all_masks.cpu().numpy(), plt.gca(), random_color=True)\n",
        "\n",
        "\n",
        "#                 def show_points(coords, labels, ax, marker_size=375):\n",
        "#                     pos_points = coords[labels==1]\n",
        "#                     neg_points = coords[labels==0]\n",
        "#                     ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "#                     ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#                 plt.figure(figsize=(10, 10))\n",
        "#                 plt.imshow(test_image)\n",
        "#                 combined_mask = np.zeros(test_image_np.shape[:2], dtype=np.uint8)\n",
        "#                 for mask in masks:\n",
        "#                     mask_np = mask.cpu().numpy().squeeze()\n",
        "#                     combined_mask = np.maximum(combined_mask, mask_np)\n",
        "#                     show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)\n",
        "#                 for box in input_boxes:\n",
        "#                     show_box(box.cpu().numpy(), plt.gca())\n",
        "#                 plt.axis('off')\n",
        "#                 plt.show()\n",
        "\n",
        "# # Convert the selected mask to a binary mask\n",
        "#                 binary_mask = combined_mask.astype(np.uint8)\n",
        "\n",
        "# # Convert binary mask to PIL image\n",
        "#                 binary_mask_pil = Image.fromarray(binary_mask * 255)\n",
        "\n",
        "# # Save the binary mask\n",
        "#                 output_path = os.path.join(output_directory, os.path.basename(test_image_filename))\n",
        "#                 binary_mask_pil.save(output_path)\n",
        "#                 print(f\"Saved mask to: {output_path}\")\n",
        "\n",
        "                # # Convert to binary mask\n",
        "                # binary_mask = (masks[2] > 0.5).astype(np.uint8)\n",
        "\n",
        "                # # Convert binary mask to PIL image\n",
        "                # binary_mask_pil = Image.fromarray(binary_mask * 255)\n",
        "\n",
        "                # # Create a draw object\n",
        "\n",
        "                # # Draw the points on the mask\n",
        "\n",
        "\n",
        "                # # Save the binary mask\n",
        "                # output_path = os.path.join(output_directory, os.path.basename(test_image_filename))\n",
        "                # binary_mask_pil.save(output_path)\n",
        "                # print(f\"Saved mask to: {output_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the results is stored you can also check the iou score with its Ground Truth"
      ],
      "metadata": {
        "id": "F7E6P5NPewFS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pDmRwr-Xf0ZF",
        "outputId": "83fdabbc-d6e2-4512-b9fc-647c81fe07c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘=’: File exists\n",
            "mkdir: cannot create directory ‘/content/drive/MyDrive/IOU-NEW’: File exists\n",
            "Individual IoU scores and average IoU saved to /content/drive/MyDrive/IOU-NEW/iou_scores_40.txt\n"
          ]
        }
      ],
      "source": [
        "#IOU SCORE.\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def calculate_iou(ground_truth, prediction):\n",
        "    # Convert binary masks to float (0.0 or 1.0)\n",
        "    gt = ground_truth.astype(np.float32)\n",
        "    pred = prediction.astype(np.float32)\n",
        "\n",
        "    if gt.shape != pred.shape:\n",
        "      pred = np.array(Image.fromarray(pred).resize(gt.shape[::-1], Image.NEAREST))\n",
        "\n",
        "    # Add both masks\n",
        "    combined = gt + pred\n",
        "\n",
        "    # Calculate number of 2's (intersection) and number of 1's (union - intersection)\n",
        "    intersection = np.sum(combined == 2)\n",
        "    union = np.sum(combined >= 1)\n",
        "\n",
        "    # Calculate IoU score\n",
        "    iou = intersection / union if union != 0 else 0\n",
        "    return iou\n",
        "\n",
        "def find_file_with_extension(directory, basename, extensions):\n",
        "    for ext in extensions:\n",
        "        filename = f\"{basename}{ext}\"\n",
        "        if os.path.isfile(os.path.join(directory, filename)):\n",
        "            return filename\n",
        "    return None\n",
        "\n",
        "def main2():\n",
        "    # Directories containing the ground truth and predicted masks\n",
        "    ground_truth_dir = \"/content/drive/MyDrive/ground_truth/040 Monks-LAO PDR 2008-Rolandito\"\n",
        "    predicted_dir = \"/content/drive/MyDrive/OUT-PUT-MASK/40\"\n",
        "    extensions = ['.jpg', '.png']\n",
        "\n",
        "    !mkdir = \"/content/drive/MyDrive/IOU-NEW\"\n",
        "    output_file = \"/content/drive/MyDrive/IOU-NEW/iou_scores_40.txt\"\n",
        "    # List of mask filenames (assuming filenames are the same in both directories)\n",
        "    ground_truth_files = [f for f in os.listdir(ground_truth_dir) if os.path.isfile(os.path.join(ground_truth_dir, f)) and (f.endswith('.jpg') or f.endswith('.png'))]\n",
        "    predicted_files = [f for f in os.listdir(predicted_dir) if os.path.isfile(os.path.join(predicted_dir, f)) and (f.endswith('.jpg') or f.endswith('.png'))]\n",
        "\n",
        "    ground_truth_basenames = {os.path.splitext(f)[0]: f for f in ground_truth_files}\n",
        "    predicted_basenames = {os.path.splitext(f)[0]: f for f in predicted_files}\n",
        "\n",
        "    # Check if both directories contain the same number of masks\n",
        "    assert len(ground_truth_files) == len(predicted_files), \"Mismatch in the number of masks between the two directories\"\n",
        "\n",
        "    # Initialize a list to store individual IoU scores\n",
        "    iou_scores = []\n",
        "\n",
        "    # Calculate IoU for each pair of masks\n",
        "    for basename, gt_filename in ground_truth_basenames.items():\n",
        "      pred_filename = find_file_with_extension(predicted_dir, basename, extensions)\n",
        "      if not pred_filename:\n",
        "            print(f\"Corresponding predicted file for {gt_filename} not found.\")\n",
        "            continue\n",
        "      gt_mask = np.array(Image.open(os.path.join(ground_truth_dir, gt_filename)).convert('L'))\n",
        "      pred_mask = np.array(Image.open(os.path.join(predicted_dir, pred_filename)).convert('L'))\n",
        "\n",
        "        # Convert to binary masks (assuming threshold at 128)\n",
        "      gt_mask = (gt_mask >= 128).astype(np.float32)\n",
        "      pred_mask = (pred_mask >= 128).astype(np.float32)\n",
        "\n",
        "        # Calculate IoU score\n",
        "      iou = calculate_iou(gt_mask, pred_mask)\n",
        "      iou_scores.append((basename, iou))\n",
        "\n",
        "    # Calculate average IoU\n",
        "    average_iou = np.mean([score for _, score in iou_scores])\n",
        "\n",
        "    # Save individual IoU scores and final average to a text file\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        for filename, score in iou_scores:\n",
        "            f.write(f\"{filename}: {score:.4f}\\n\")\n",
        "        f.write(f\"\\nAverage IoU: {average_iou:.4f}\\n\")\n",
        "\n",
        "    print(f\"Individual IoU scores and average IoU saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTKyli8Pyyry",
        "outputId": "3488c7fa-3bc2-4c61-948f-e770d85a044d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted non-empty folder: /content/Output/50_2\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def delete_non_empty_folder(folder_path):\n",
        "    if os.path.exists(folder_path):\n",
        "        if os.path.islink(folder_path):  # Check if it's a symbolic link\n",
        "            os.unlink(folder_path)  # Remove the symbolic link itself\n",
        "            print(f\"Deleted symbolic link: {folder_path}\")\n",
        "        elif os.path.isdir(folder_path):\n",
        "            shutil.rmtree(folder_path)  # Use rmtree for directories\n",
        "            print(f\"Deleted non-empty folder: {folder_path}\")\n",
        "        else:\n",
        "            print(f\"Path is not a folder or symbolic link: {folder_path}\")\n",
        "    else:\n",
        "        print(f\"Path does not exist: {folder_path}\")\n",
        "\n",
        "# Usage\n",
        "folder_to_delete = '/content/Output/50_2'\n",
        "delete_non_empty_folder(folder_to_delete)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA4CPD82nDr3",
        "outputId": "6e276a55-cfaf-4ee1-abd5-1b9647319e07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to final_iou_average.txt\n",
            "Overall Average IoU: 0.6626\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def extract_average_iou(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        # The last line contains the average IoU\n",
        "        for line in lines:\n",
        "            if \"Average IoU\" in line:\n",
        "                avg_iou = float(line.split(\":\")[-1].strip())\n",
        "                return avg_iou\n",
        "    return None\n",
        "\n",
        "def main3():\n",
        "    # Directory containing the IoU score text files\n",
        "    iou_scores_dir = \"/content/drive/MyDrive/IOU_SCORE\"  # Replace with your directory path\n",
        "    iou_files = [f for f in os.listdir(iou_scores_dir) if f.endswith('.txt')]\n",
        "\n",
        "    # List to store average IoU scores from each file\n",
        "    average_ious = []\n",
        "\n",
        "    for iou_file in iou_files:\n",
        "        iou_file_path = os.path.join(iou_scores_dir, iou_file)\n",
        "        avg_iou = extract_average_iou(iou_file_path)\n",
        "        if avg_iou is not None:\n",
        "            average_ious.append(avg_iou)\n",
        "\n",
        "    # Calculate the overall average IoU\n",
        "    if average_ious:\n",
        "        overall_average_iou = sum(average_ious) / len(average_ious)\n",
        "\n",
        "        output_file = \"final_iou_average.txt\"\n",
        "        with open(output_file, 'w') as f:\n",
        "            f.write(f\"Overall Average IoU: {overall_average_iou:.4f}\\n\")\n",
        "            f.write(f\"Total number of IoU scores: {len(average_ious)}\\n\")\n",
        "\n",
        "        print(f\"Results saved to {output_file}\")\n",
        "        print(f\"Overall Average IoU: {overall_average_iou:.4f}\")\n",
        "    else:\n",
        "        print(\"No valid IoU scores found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main3()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOp9YeSopHT9",
        "outputId": "5939eb51-983e-494c-e696-f6c180dc3544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Average IoU: 0.6363\n",
            "Total number of IoU scores: 595\n",
            "Results saved to final_iou_average_per_image.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def extract_individual_ious(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        ious = []\n",
        "        for line in lines:\n",
        "            if \"Average IoU\" in line:\n",
        "                continue\n",
        "            if \":\" in line:\n",
        "                iou = float(line.split(\":\")[-1].strip())\n",
        "                ious.append(iou)\n",
        "    return ious\n",
        "\n",
        "def main4():\n",
        "    # Directory containing the IoU score text files\n",
        "    iou_scores_dir = \"/content/drive/MyDrive/IOU_SCORE\"  # Replace with your directory path\n",
        "    iou_files = [f for f in os.listdir(iou_scores_dir) if f.endswith('.txt')]\n",
        "\n",
        "    # List to store individual IoU scores from each file\n",
        "    all_ious = []\n",
        "\n",
        "    for iou_file in iou_files:\n",
        "        iou_file_path = os.path.join(iou_scores_dir, iou_file)\n",
        "        ious = extract_individual_ious(iou_file_path)\n",
        "        if ious:\n",
        "            all_ious.extend(ious)\n",
        "\n",
        "    # Calculate the overall average IoU\n",
        "    if all_ious:\n",
        "        overall_average_iou = sum(all_ious) / len(all_ious)\n",
        "        print(f\"Overall Average IoU: {overall_average_iou:.4f}\")\n",
        "        print(f\"Total number of IoU scores: {len(all_ious)}\")\n",
        "\n",
        "        # Save the result to a text file\n",
        "        output_file = \"final_iou_average_per_image.txt\"\n",
        "        with open(output_file, 'w') as f:\n",
        "            f.write(f\"Overall Average IoU: {overall_average_iou:.4f}\\n\")\n",
        "            f.write(f\"Total number of IoU scores: {len(all_ious)}\\n\")\n",
        "\n",
        "        print(f\"Results saved to {output_file}\")\n",
        "    else:\n",
        "        print(\"No valid IoU scores found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main4()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOjrNl3_sk7U",
        "outputId": "8701453d-7900-4f5e-9f81-e80c0443f3dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to folder_iou_statistics2.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def extract_average_iou(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        # The last line contains the average IoU\n",
        "        for line in lines:\n",
        "            if \"Average IoU\" in line:\n",
        "                avg_iou = float(line.split(\":\")[-1].strip())\n",
        "                return avg_iou\n",
        "    return None\n",
        "\n",
        "def main():\n",
        "    # Directory containing the IoU score text files\n",
        "    iou_scores_dir = \"/content/drive/MyDrive/IOU_SCORE\"  # Replace with your directory path\n",
        "    iou_files = [f for f in os.listdir(iou_scores_dir) if f.endswith('.txt')]\n",
        "\n",
        "    # List to store average IoU scores from each file\n",
        "    average_ious = []\n",
        "    num_folders = 0\n",
        "    num_folders_above_09 = 0\n",
        "    num_folders_above_08 = 0\n",
        "    num_folders_above_07 = 0\n",
        "    num_folders_above_06 = 0\n",
        "\n",
        "    # Lists to store average IoU for folders with IoU > 0.9, 0.8, 0.7, and 0.6\n",
        "    avg_iou_above_09 = []\n",
        "    avg_iou_above_08 = []\n",
        "    avg_iou_above_07 = []\n",
        "    avg_iou_above_06 = []\n",
        "\n",
        "    for iou_file in iou_files:\n",
        "        iou_file_path = os.path.join(iou_scores_dir, iou_file)\n",
        "        avg_iou = extract_average_iou(iou_file_path)\n",
        "        if avg_iou is not None:\n",
        "            average_ious.append(avg_iou)\n",
        "            num_folders += 1\n",
        "            if avg_iou > 0.9:\n",
        "                num_folders_above_09 += 1\n",
        "                avg_iou_above_09.append(avg_iou)\n",
        "            if avg_iou > 0.8:\n",
        "                num_folders_above_08 += 1\n",
        "                avg_iou_above_08.append(avg_iou)\n",
        "            if avg_iou > 0.7:\n",
        "                num_folders_above_07 += 1\n",
        "                avg_iou_above_07.append(avg_iou)\n",
        "            if avg_iou > 0.6:\n",
        "                num_folders_above_06 += 1\n",
        "                avg_iou_above_06.append(avg_iou)\n",
        "\n",
        "    # Calculate the total average IoU\n",
        "    if average_ious:\n",
        "        total_average_iou = sum(average_ious) / len(average_ious)\n",
        "        # Calculate average IoU for folders with IoU > 0.9, 0.8, 0.7, and 0.6\n",
        "        avg_iou_09 = sum(avg_iou_above_09) / len(avg_iou_above_09) if avg_iou_above_09 else 0\n",
        "        avg_iou_08 = sum(avg_iou_above_08) / len(avg_iou_above_08) if avg_iou_above_08 else 0\n",
        "        avg_iou_07 = sum(avg_iou_above_07) / len(avg_iou_above_07) if avg_iou_above_07 else 0\n",
        "        avg_iou_06 = sum(avg_iou_above_06) / len(avg_iou_above_06) if avg_iou_above_06 else 0\n",
        "\n",
        "        # Save the result to a text file\n",
        "        output_file = \"folder_iou_statistics2.txt\"\n",
        "        with open(output_file, 'w') as f:\n",
        "            f.write(f\"Total Average IoU: {total_average_iou:.4f}\\n\")\n",
        "            f.write(f\"Total number of folders: {num_folders}\\n\")\n",
        "            f.write(f\"Number of folders with IoU > 0.9: {num_folders_above_09}\\n\")\n",
        "            f.write(f\"Average IoU for folders with IoU > 0.9: {avg_iou_09:.4f}\\n\")\n",
        "            f.write(f\"Number of folders with IoU > 0.8: {num_folders_above_08}\\n\")\n",
        "            f.write(f\"Average IoU for folders with IoU > 0.8: {avg_iou_08:.4f}\\n\")\n",
        "            f.write(f\"Number of folders with IoU > 0.7: {num_folders_above_07}\\n\")\n",
        "            f.write(f\"Average IoU for folders with IoU > 0.7: {avg_iou_07:.4f}\\n\")\n",
        "            f.write(f\"Number of folders with IoU > 0.6: {num_folders_above_06}\\n\")\n",
        "            f.write(f\"Average IoU for folders with IoU > 0.6: {avg_iou_06:.4f}\\n\")\n",
        "\n",
        "        print(f\"Results saved to {output_file}\")\n",
        "    else:\n",
        "        print(\"No valid IoU scores found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d10ffae93aa04746b41ee4ee81f278a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6c6480c9b2441af9b903c65a5d6cf2c",
              "IPY_MODEL_179f4f7c03854baca5db9f4afedbf560",
              "IPY_MODEL_cfb9b66a73934b95af3da7025cc7f1c6"
            ],
            "layout": "IPY_MODEL_2168996fe94e491e83d61da98bb22843"
          }
        },
        "e6c6480c9b2441af9b903c65a5d6cf2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2864d7864dee49caa494999e4d992d24",
            "placeholder": "​",
            "style": "IPY_MODEL_f7dd1ccdddcb45efb83e190a884d3366",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "179f4f7c03854baca5db9f4afedbf560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8873cbfa52a46be8143df7130957866",
            "max": 466,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22c4a33df36f48149350a88809ddd0e0",
            "value": 466
          }
        },
        "cfb9b66a73934b95af3da7025cc7f1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37d86c30cb3c4644af6dbf4b4a4dc5ca",
            "placeholder": "​",
            "style": "IPY_MODEL_63027011d8024f9680b9ffbae6f344ec",
            "value": " 466/466 [00:00&lt;00:00, 19.5kB/s]"
          }
        },
        "2168996fe94e491e83d61da98bb22843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2864d7864dee49caa494999e4d992d24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7dd1ccdddcb45efb83e190a884d3366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8873cbfa52a46be8143df7130957866": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22c4a33df36f48149350a88809ddd0e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37d86c30cb3c4644af6dbf4b4a4dc5ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63027011d8024f9680b9ffbae6f344ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f79b832b8204111967517da93c4a263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ece7500d65e844a49683bf6b4374f3d6",
              "IPY_MODEL_a2976df6649f416b9c9fcdd0a7370aad",
              "IPY_MODEL_3844e18c87604cbb974fcb14c7460e53"
            ],
            "layout": "IPY_MODEL_f5759a04390445089d339c9e20453698"
          }
        },
        "ece7500d65e844a49683bf6b4374f3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d9456e94af644adbfa5334163e3c478",
            "placeholder": "​",
            "style": "IPY_MODEL_c9275e44f29b4a57b9692a454247726c",
            "value": "config.json: 100%"
          }
        },
        "a2976df6649f416b9c9fcdd0a7370aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d60fadf7c2a04fd09e74502f82b6806c",
            "max": 6569,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_383011f39a6448a09a7d76b7178f3b2f",
            "value": 6569
          }
        },
        "3844e18c87604cbb974fcb14c7460e53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e916858f179471b85579baa9407e9ba",
            "placeholder": "​",
            "style": "IPY_MODEL_46d6e9a021fb4682bccf61d52f8076c3",
            "value": " 6.57k/6.57k [00:00&lt;00:00, 551kB/s]"
          }
        },
        "f5759a04390445089d339c9e20453698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d9456e94af644adbfa5334163e3c478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9275e44f29b4a57b9692a454247726c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d60fadf7c2a04fd09e74502f82b6806c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "383011f39a6448a09a7d76b7178f3b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e916858f179471b85579baa9407e9ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46d6e9a021fb4682bccf61d52f8076c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "522cd35a9c59403999b8e5e793226945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df9ec6456a09491da2ae39099facebe6",
              "IPY_MODEL_8f52cf77b134456486d0fef740750459",
              "IPY_MODEL_5e70354cfae0494999ffa158a09cd2ba"
            ],
            "layout": "IPY_MODEL_0779502b51214262912eaec3eac732a6"
          }
        },
        "df9ec6456a09491da2ae39099facebe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d4f70a41aed44918cd2c0dec4b87f4d",
            "placeholder": "​",
            "style": "IPY_MODEL_ef02f947198041eea185793acb7d3dcd",
            "value": "model.safetensors: 100%"
          }
        },
        "8f52cf77b134456486d0fef740750459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76b4fa248f4243dc99e9af166ebeded7",
            "max": 2564432288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d04d1d153b24352a6a498967ba86290",
            "value": 2564432288
          }
        },
        "5e70354cfae0494999ffa158a09cd2ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da8a3e3861834a6cbff9ef2bf7839dce",
            "placeholder": "​",
            "style": "IPY_MODEL_6724291c26e149908db4d9bc02242bf1",
            "value": " 2.56G/2.56G [00:13&lt;00:00, 224MB/s]"
          }
        },
        "0779502b51214262912eaec3eac732a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d4f70a41aed44918cd2c0dec4b87f4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef02f947198041eea185793acb7d3dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76b4fa248f4243dc99e9af166ebeded7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d04d1d153b24352a6a498967ba86290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da8a3e3861834a6cbff9ef2bf7839dce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6724291c26e149908db4d9bc02242bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}